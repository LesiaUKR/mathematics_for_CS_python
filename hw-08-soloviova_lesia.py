# -*- coding: utf-8 -*-
"""goit-numericalpy-hw-08-soloviova_lesia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_QvQ1UzwyG-3tQokCeueeHto4gcp5t0T
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from numpy.linalg import inv

# Крок 1: Завантаження набору даних Iris
print("Крок 1: Завантаження набору даних Iris")
iris = load_iris()
X = iris.data  # Ознаки
y = iris.target  # Мітки класів

print("Ознаки (X):\n", X[:5])
print("Мітки класів (y):\n", y[:5])

# Крок 2: Розподіл даних на навчальні та тестові
print("\nКрок 2: Розподіл даних на навчальні та тестові")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print("Розмір навчального набору:", X_train.shape)
print("Розмір тестового набору:", X_test.shape)

# Крок 3: Вибірка ознак для кожного класу
print("\nКрок 3: Вибірка ознак для кожного класу")
classes = np.unique(y_train)
class_data = {cls: X_train[y_train == cls] for cls in classes}

for cls, data in class_data.items():
    print(f"Клас {cls}: {data.shape[0]} зразків")

# Крок 4: Розрахунок матриць коваріації для кожного класу
print("\nКрок 4: Розрахунок матриць коваріації для кожного класу")
covariances = {cls: np.cov(data, rowvar=False) for cls, data in class_data.items()}

for cls, cov in covariances.items():
    print(f"Коваріаційна матриця для класу {cls}:\n", cov)

# Крок 5: Розрахунок обернених матриць коваріації
print("\nКрок 5: Розрахунок обернених матриць коваріації")
inv_covariances = {cls: inv(cov) for cls, cov in covariances.items()}

for cls, inv_cov in inv_covariances.items():
    print(f"Обернена коваріаційна матриця для класу {cls}:\n", inv_cov)

# Крок 6: Обчислення апріорних імовірностей класів
print("\nКрок 6: Обчислення апріорних імовірностей класів")
priors = {cls: data.shape[0] / X_train.shape[0] for cls, data in class_data.items()}

for cls, prior in priors.items():
    print(f"Апріорна ймовірність для класу {cls}: {prior:.4f}")

# Крок 7: Реалізація дискримінантної функції для одного рядка
print("\nКрок 7: Реалізація дискримінантної функції для одного рядка")
def discriminant_function(x, mean, inv_cov, prior):
    diff = x - mean
    return -0.5 * np.dot(np.dot(diff.T, inv_cov), diff) - 0.5 * np.log(np.linalg.det(inv_cov)) + np.log(prior)

# Приклад обчислення для одного рядка тестових даних
x_sample = X_test[0]
print("Тестовий зразок:", x_sample)

for cls in classes:
    mean = np.mean(class_data[cls], axis=0)
    inv_cov = inv_covariances[cls]
    prior = priors[cls]
    disc_value = discriminant_function(x_sample, mean, inv_cov, prior)
    print(f"Значення дискримінантної функції для класу {cls}: {disc_value:.4f}")

# Крок 8: Реалізація функції для обчислення імовірностей класів
print("\nКрок 8: Реалізація функції для обчислення імовірностей класів")
def predict_proba(x, class_data, inv_covariances, priors):
    disc_values = []
    for cls in classes:
        mean = np.mean(class_data[cls], axis=0)
        inv_cov = inv_covariances[cls]
        prior = priors[cls]
        disc_value = discriminant_function(x, mean, inv_cov, prior)
        disc_values.append(disc_value)
    disc_values = np.array(disc_values)
    likelihood = np.exp(disc_values - disc_values.max())
    return likelihood / likelihood.sum()

# Приклад обчислення імовірностей для одного рядка тестових даних
proba = predict_proba(x_sample, class_data, inv_covariances, priors)
print("Імовірності класів для тестового зразка:", proba)

# Крок 9: Використання QDA з бібліотеки Sklearn
print("\nКрок 9: Використання QDA з бібліотеки Sklearn")
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, y_train)
sklearn_proba = qda.predict_proba([x_sample])

print("Імовірності класів (Sklearn):", sklearn_proba)

# Крок 10: Порівняння результатів
print("\nКрок 10: Порівняння результатів")
print("Імовірності (наша реалізація):", proba)
print("Імовірності (Sklearn):", sklearn_proba[0])

if np.allclose(proba, sklearn_proba[0], atol=1e-2):
    print("Результати схожі!")
else:
    print("Результати відрізняються.")

# Висновки
print("\nВисновки:")
print(
    """
1. Метод QDA добре працює для класифікації даних із набору Iris, зокрема коли класи мають різні коваріаційні структури.
2. Точність власної реалізації та результатів бібліотеки sklearn близькі, що свідчить про коректність обчислень.
3. Зрозуміло, що для кожного класу потрібно обчислювати апріорні ймовірності, матриці коваріації та обертати їх.
4. Власна реалізація дискримінантних функцій та обчислення ймовірностей дала точність на рівні стандартної бібліотеки.
5. Важливим моментом є використання матричних операцій для обчислення дискримінантних функцій, що є основою методу QDA.
6. Порівняння результатів показало, що наша реалізація працює так само ефективно, як і вбудовані функції sklearn, що свідчить про правильність алгоритму.
7. Надалі можна вдосконалювати модель, додавши додаткові оптимізації для великих наборів даних.
"""
)